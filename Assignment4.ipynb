{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Deep learning and unsupervised learning\n",
    "For this assignment you are allowed to use data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Pick any image based dataset from the list, implement the preprocessing and justify the preprocessing steps, extract features and justify the methods used, select features and justify the methods used. Some of this is done already in one of the previous assignments. You can reuse\n",
    "things.\n",
    "\n",
    "- [] Implement (using the selected features) one basic machine learning algorithm for classification and justify your choice 20 (without justification 10).\n",
    "\n",
    "- [] Implement (using the selected features) one advanced machine learning algorithm for classification and justify your choice 20 (without justification 10).\n",
    "\n",
    "- [] Implement a CNN with hyperparameter tuning (for this you can directly use the data after the preprocessing) (30)\n",
    "\n",
    "- [] Compare and Explain the results in terms of both the computation time and the performance of the classification algorithms. (30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing, extract features, select features.\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define paths and parameters\n",
    "dataset_path = \"image_dataset\"\n",
    "preprocessed_path = \"preprocessed_arrays\"\n",
    "image_size = (224, 224)  # Image size\n",
    "class_names = [\"hatchback\", \"motorcycle\", \"pickup\", \"sedan\", \"suv\"]\n",
    "\n",
    "# Create directory to save preprocessed arrays\n",
    "os.makedirs(preprocessed_path, exist_ok=True)\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    Preprocess the image: convert to grayscale, resize, and normalize.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Step 2: Resize the image\n",
    "    img_resized = cv2.resize(gray, image_size)\n",
    "    # Step 3: Normalize the image\n",
    "    img_normalized = img_resized / 255.0\n",
    "    return img_normalized\n",
    "\n",
    "# Preprocess images and save as NumPy arrays\n",
    "for class_name in class_names:\n",
    "    folder_path = os.path.join(dataset_path, class_name)\n",
    "    class_output_path = os.path.join(preprocessed_path, class_name)\n",
    "    os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                # Read and preprocess the image\n",
    "                img = cv2.imread(file_path)\n",
    "                img_preprocessed = preprocess_image(img)\n",
    "\n",
    "                # Save the preprocessed image as a NumPy array\n",
    "                save_path = os.path.join(class_output_path, file_name.split('.')[0] + \".npy\")\n",
    "                np.save(save_path, img_preprocessed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "print(f\"Preprocessing completed. Preprocessed arrays saved in '{preprocessed_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "# Define paths\n",
    "edge_detected_path = \"edge_detected_arrays\"\n",
    "\n",
    "# Create directory to save edge-detected arrays\n",
    "os.makedirs(edge_detected_path, exist_ok=True)\n",
    "\n",
    "def apply_edge_detection(img):\n",
    "    \"\"\"\n",
    "    Apply edge detection to a preprocessed image (already in grayscale).\n",
    "    \"\"\"\n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    # Apply Canny Edge Detection\n",
    "    edges = cv2.Canny((blurred * 255).astype(np.uint8), threshold1=50, threshold2=150)\n",
    "    return edges / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Perform edge detection and save as NumPy arrays\n",
    "for class_name in class_names:\n",
    "    class_input_path = os.path.join(preprocessed_path, class_name)\n",
    "    class_output_path = os.path.join(edge_detected_path, class_name)\n",
    "    os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(class_input_path):\n",
    "        print(f\"Folder not found: {class_input_path}\")\n",
    "        continue\n",
    "\n",
    "    for file_name in os.listdir(class_input_path):\n",
    "        file_path = os.path.join(class_input_path, file_name)\n",
    "\n",
    "        if file_name.lower().endswith('.npy'):\n",
    "            try:\n",
    "                # Load the preprocessed image as a NumPy array\n",
    "                img_preprocessed = np.load(file_path)\n",
    "                # Apply edge detection\n",
    "                img_edges = apply_edge_detection(img_preprocessed)\n",
    "\n",
    "                # Save the edge-detected image as a NumPy array\n",
    "                save_path = os.path.join(class_output_path, file_name)\n",
    "                np.save(save_path, img_edges)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "print(f\"Edge detection completed. Edge-detected arrays saved in '{edge_detected_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Define paths\n",
    "preprocessed_path = \"edge_detected_arrays\"\n",
    "feature_selected_path = \"feature_selected_arrays\"\n",
    "\n",
    "# Create directory to save feature-selected arrays\n",
    "os.makedirs(feature_selected_path, exist_ok=True)\n",
    "\n",
    "# Load preprocessed data\n",
    "X = []  # Feature data\n",
    "y = []  # Labels\n",
    "\n",
    "for class_name in sorted(os.listdir(preprocessed_path)):  # Sort class directories\n",
    "    class_input_path = os.path.join(preprocessed_path, class_name)\n",
    "\n",
    "    if not os.path.exists(class_input_path):\n",
    "        print(f\"Folder not found: {class_input_path}\")\n",
    "        continue\n",
    "\n",
    "    for file_name in sorted(os.listdir(class_input_path)):  # Sort files within each class\n",
    "        if file_name.lower().endswith('.npy'):\n",
    "            try:\n",
    "                # Load preprocessed image\n",
    "                file_path = os.path.join(class_input_path, file_name)\n",
    "                img_array = np.load(file_path).flatten()  # Flatten image array\n",
    "                X.append(img_array)\n",
    "                y.append(class_name)  # Add corresponding class label\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Define the range of components to test\n",
    "components_range = [5, 15, 16, 17, 18, 19, 20]#, 50, 100, 300, 900]\n",
    "\n",
    "# Dictionary to store cross-validation results\n",
    "results = {}\n",
    "\n",
    "# Loop over different numbers of PCA components\n",
    "for n_components in components_range:\n",
    "    # Create a pipeline that applies PCA and then a classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=n_components)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation and compute the mean accuracy\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "    results[n_components] = np.mean(cv_scores)\n",
    "    print(f\"Components: {n_components}, CV Accuracy: {results[n_components]:.4f}\")\n",
    "\n",
    "# Find the optimal number of components based on cross-validation accuracy\n",
    "best_n_components = max(results, key=results.get)\n",
    "print(f\"Optimal number of components: {best_n_components}\")\n",
    "print(f\"Best cross-validated accuracy: {results[best_n_components]:.4f}\")\n",
    "\n",
    "pca = PCA(n_components=best_n_components)\n",
    "image_data_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_data_reduced, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Basic Classification (Random Forest)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optional: Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Advanced Classification (SVM)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Optimized SVM Model Accuracy: {accuracy_best * 100:.2f}%\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Optional: Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Define paths\n",
    "# preprocessed_path = \"preprocessed_arrays\"\n",
    "\n",
    "# # Load preprocessed data\n",
    "# X = []  # Feature data\n",
    "# y = []  # Labels\n",
    "\n",
    "# for class_name in sorted(os.listdir(preprocessed_path)):  # Sort class directories\n",
    "#     class_input_path = os.path.join(preprocessed_path, class_name)\n",
    "\n",
    "#     if not os.path.exists(class_input_path):\n",
    "#         print(f\"Folder not found: {class_input_path}\")\n",
    "#         continue\n",
    "\n",
    "#     for file_name in sorted(os.listdir(class_input_path)):  # Sort files within each class\n",
    "#         if file_name.lower().endswith('.npy'):\n",
    "#             try:\n",
    "#                 # Load preprocessed image\n",
    "#                 file_path = os.path.join(class_input_path, file_name)\n",
    "#                 img_array = np.load(file_path)  # Do not flatten for CNN\n",
    "#                 X.append(img_array)\n",
    "#                 y.append(class_name)  # Add corresponding class label\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "# # Convert to NumPy arrays\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# # Add channel dimension\n",
    "# X = np.expand_dims(X, axis=-1)  # Shape becomes (batch_size, height, width, 1)\n",
    "\n",
    "# # Encode labels\n",
    "# unique_classes = sorted(set(y))\n",
    "# label_to_index = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "# y = np.array([label_to_index[label] for label in y])\n",
    "\n",
    "# # Convert labels to one-hot encoding\n",
    "# y = to_categorical(y, num_classes=len(unique_classes))\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# # Define data augmentation\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=20,       # Rotate images by up to 20 degrees\n",
    "#     width_shift_range=0.2,   # Shift images horizontally by 20% of the width\n",
    "#     height_shift_range=0.2,  # Shift images vertically by 20% of the height\n",
    "#     zoom_range=0.2,          # Randomly zoom in/out\n",
    "#     horizontal_flip=True,    # Flip images horizontally\n",
    "#     fill_mode='nearest'      # Fill missing pixels with nearest values\n",
    "# )\n",
    "\n",
    "# # Fit the data generator to the training data\n",
    "# datagen.fit(X_train)\n",
    "\n",
    "# # Hyperparameters\n",
    "# num_filters = 64\n",
    "# kernel_size = (3, 3)\n",
    "# pool_size = (2, 2)\n",
    "# dense_units = 128\n",
    "# dropout_rate = 0.3\n",
    "# input_shape = X_train.shape[1:]  # Shape of a single image\n",
    "\n",
    "# # Build CNN model\n",
    "# model = Sequential([\n",
    "#     Conv2D(num_filters, kernel_size, activation='relu', input_shape=input_shape),\n",
    "#     MaxPooling2D(pool_size=pool_size),\n",
    "#     Conv2D(num_filters * 2, kernel_size, activation='relu'),\n",
    "#     MaxPooling2D(pool_size=pool_size),\n",
    "#     Flatten(),\n",
    "#     Dense(dense_units, activation='relu'),\n",
    "#     Dropout(dropout_rate),\n",
    "#     Dense(len(unique_classes), activation='softmax')  # Output layer\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model with data augmentation\n",
    "# batch_size = 32\n",
    "# epochs = 20\n",
    "# history = model.fit(\n",
    "#     datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "#     validation_data=(X_test, y_test),\n",
    "#     epochs=epochs\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"cnn_model_with_augmentation.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Pick any dataset from the list, implement the preprocessing and justify the preprocessing steps, extract features and justify the methods used, select features and justify the methods used. Some of this is done already in one of the previous assignments. You can reuse things.\n",
    "\n",
    "Implement three clustering methods out of the following and justify your choices (30)\n",
    "\n",
    "- K-means\n",
    "- Hierarchical Clustering\n",
    "- Fuzzy-C-means\n",
    "- DBSCAN\n",
    "- Gaussian mixture models\n",
    "- Self-organizing maps\n",
    "\n",
    "Compare and Explain the results (30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing, extract features, select features. (Can reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement cluster method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement cluster method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement cluster method 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
